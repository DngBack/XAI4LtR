{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1199870,"sourceType":"datasetVersion","datasetId":615374},{"sourceId":1412771,"sourceType":"datasetVersion","datasetId":824374}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, time, json, random, glob, copy\nimport numpy as np, cv2, matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom torchvision import models\nfrom albumentations import Compose, Resize, RandomBrightnessContrast, Rotate, HorizontalFlip, Normalize\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.cuda.amp import autocast, GradScaler # For mixed precision training\n\n# ---------------------------\n# 0. Thiết lập tham số chung\n# ---------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nnum_classes = 2\n\n# Loss weights & thresholds\nlambda_unc   = 0.5   # Weight for evidential uncertainty loss\ntarget_risk  = 0.02  # Max allowed risk on kept samples (Currently not used directly, but good for context)\nalpha_conf   = 0.05  # Conformal prediction error rate\n\nlearning_rate_p1  = 1e-4\nlearning_rate_p2  = 1e-3 # Tăng learning rate cho pha 2 vì chỉ huấn luyện head nhỏ\nbatch_size_p1     = 8   # Batch size cho pha 1\nbatch_size_p2     = 64  # Tăng batch size cho pha 2 (vì chỉ xử lý features, không phải ảnh gốc)\naccumulation_steps = 2 # Gradient accumulation\nnum_epochs_p1     = 10\nnum_epochs_p2     = 10 # Reduced epochs for lighter phase 2\n\n# Paths\npositive_folder  = \"/kaggle/input/sarscov2-ctscan-dataset/COVID\"\nnegative_folder  = \"/kaggle/input/sarscov2-ctscan-dataset/non-COVID\"\ncheckpoint_dir   = \"checkpoints\"\nos.makedirs(checkpoint_dir, exist_ok=True)\n\n# Set environment variable to reduce memory fragmentation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# ---------------------------\n# 1. Load & split data\n# ---------------------------\ndef load_data_paths():\n    pos = glob.glob(os.path.join(positive_folder, \"*.png\"))\n    neg = glob.glob(os.path.join(negative_folder, \"*.png\"))\n    random.shuffle(pos); random.shuffle(neg)\n    split_p = int(0.8 * len(pos))\n    split_n = int(0.8 * len(neg))\n    return pos[:split_p], neg[:split_n], pos[split_p:], neg[split_n:]\n\ntrain_pos, train_neg, test_pos, test_neg = load_data_paths()\n\n# ---------------------------\n# 2. Dataset\n# ---------------------------\nclass CTScanDataset(Dataset):\n    def __init__(self, pos_paths, neg_paths, transform):\n        self.paths = pos_paths + neg_paths\n        self.labels = [0] * len(pos_paths) + [1] * len(neg_paths)\n        self.transform = transform\n\n    def __len__(self): return len(self.paths)\n\n    def __getitem__(self, i):\n        p = self.paths[i]\n        l = self.labels[i]\n        img = cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB)\n        img = self.transform(image=img)[\"image\"]\n        return img, l, p # Trả về cả đường dẫn để phục vụ debugging/ghi log\n\n# ---------------------------\n# 3. Model\n# ---------------------------\nclass MobileNetV2WithUncertainty(nn.Module):\n    def __init__(self, num_classes=2, p_drop=0.5, temperature=1.0):\n        super().__init__()\n        m = models.mobilenet_v2(pretrained=True)\n        # Thêm dropout vào các layer nhất định trong backbone\n        # Tắt tự động thêm dropout bằng cách thêm dropout ở cuối mỗi block invert_residual\n        # for idx, layer in enumerate(m.features):\n        #     if idx % 5 == 4: layer.add_module(\"drop\", nn.Dropout(p_drop)) # Ví dụ: thêm dropout sau mỗi 5 layer\n\n        self.backbone = m\n        self.backbone.classifier = nn.Identity() # Bỏ head phân loại gốc của MobileNetV2\n        self.pool = nn.AdaptiveAvgPool2d((1,1))\n        self.dropout = nn.Dropout(p_drop)\n        self.fc = nn.Linear(self.backbone.last_channel, num_classes)\n        self.unc_head = nn.Sequential(\n            nn.Linear(self.backbone.last_channel, 128), nn.ReLU(),\n            nn.Dropout(p_drop), nn.Linear(128, num_classes), nn.Softplus()\n        )\n        self.temperature = nn.Parameter(torch.tensor(temperature))\n        self.feature_grad = None\n        self.feature_map = None\n\n    def save_grad(self, grad): self.feature_grad = grad\n\n    def forward(self, x):\n        # Xử lý input có thể là ảnh hoặc feature vector\n        if x.dim() == 4: # Input là ảnh (Batch, Channels, H, W)\n            f = self.backbone.features(x)\n            p = self.pool(f).flatten(1)\n        elif x.dim() == 2: # Input là feature vector (Batch, Features)\n            p = x # Không cần qua backbone và pooling nữa\n            f = None # Không có feature map khi input là vector\n        else:\n            raise ValueError(\"Input tensor must be 2D (features) or 4D (image).\")\n\n        p = self.dropout(p)\n        logits = self.fc(p) / self.temperature\n        evidence = self.unc_head(p)\n        return logits, evidence\n\n    # Hàm forward riêng để lấy feature map cho Grad-CAM++\n    def forward_with_features(self, x):\n        f = self.backbone.features(x)\n        if f.requires_grad:\n            f.register_hook(self.save_grad)\n        self.feature_map = f.clone() # Lưu feature map để tính alpha trong Grad-CAM++\n        p = self.pool(f).flatten(1)\n        p = self.dropout(p)\n        logits = self.fc(p) / self.temperature\n        evidence = self.unc_head(p)\n        return f, logits, evidence\n\n# ---------------------------\n# 4. Helper Functions\n# ---------------------------\ndef compute_uncertainty(evidence):\n    alpha = evidence + 1\n    S = alpha.sum(dim=1, keepdim=True)\n    uncertainty = num_classes / S\n    return uncertainty\n\ndef evidential_loss(logits, evidence, y):\n    alpha = evidence + 1\n    S = alpha.sum(dim=1, keepdim=True)\n    p = alpha / S\n    target_alpha = torch.zeros_like(alpha)\n    target_alpha.scatter_(1, y.unsqueeze(1), 1)\n    kl_div = torch.sum(target_alpha * (torch.log(target_alpha + 1e-10) - torch.log(p + 1e-10)), dim=1)\n    return kl_div.mean()\n\ndef optimize_temperature(model, val_loader, device):\n    model.eval()\n    nll_criterion = nn.CrossEntropyLoss()\n    # Tối ưu hóa chỉ tham số temperature\n    optimizer = optim.LBFGS([model.temperature], lr=0.01, max_iter=50, history_size=100)\n\n    def closure():\n        optimizer.zero_grad()\n        loss = 0\n        for x, y, _ in val_loader: # Val loader vẫn chứa ảnh đầy đủ\n            x, y = x.to(device), y.to(device)\n            with autocast():\n                logits, _ = model(x)\n            loss += nll_criterion(logits, y)\n        loss.backward()\n        return loss\n\n    print(f\"Optimizing temperature (initial: {model.temperature.item():.3f})...\")\n    optimizer.step(closure)\n    print(f\"Optimized temperature: {model.temperature.item():.3f}\")\n\ndef compute_nonconformity(model, loader, device, is_features_loader=False):\n    model.eval()\n    scores = []\n    with torch.no_grad():\n        for x, y, _ in loader:\n            x, y = x.to(device), y.to(device)\n            with autocast():\n                # Dựa vào input type để gọi model\n                if is_features_loader:\n                    logits, _ = model(x) # x đã là features\n                else:\n                    logits, _ = model(x) # x là ảnh\n            probs = F.softmax(logits, dim=1)\n            # score = 1 - P(y_true)\n            scores.extend([1 - probs[i, y[i]].item() for i in range(len(y))])\n            torch.cuda.empty_cache()\n    return np.array(scores)\n\ndef set_conformal_threshold(scores, alpha=alpha_conf):\n    q = np.quantile(scores, 1 - alpha)\n    return q\n\ndef grad_cam_plus_plus(model, img_tensor, target_class):\n    # Ensure model is in eval mode and gradients are enabled for features\n    model.eval()\n    # Temporarily enable gradients for backbone.features if they were frozen\n    for param in model.backbone.features.parameters():\n        param.requires_grad = True\n\n    # Use a dummy input to clear any previous hooks/grads\n    _ = model.forward_with_features(torch.zeros_like(img_tensor).unsqueeze(0).to(device))\n    \n    # Clear previous gradients\n    if model.feature_grad is not None:\n        model.feature_grad = None\n\n    img_input = img_tensor.unsqueeze(0).to(device)\n    img_input.requires_grad_(True) # Cần gradient cho input để hook vào feature map\n\n    with autocast():\n        fmap, logits, _ = model.forward_with_features(img_input)\n    \n    # Zero out grads for logits first if needed\n    model.zero_grad() \n\n    score = logits[0, target_class]\n    score.backward()\n\n    # Get the gradient from the hook\n    grad = model.feature_grad\n    if grad is None:\n        print(\"Warning: feature_grad is None. Grad-CAM++ might not work as expected.\")\n        # Fallback or handle error\n        return np.zeros((fmap.shape[2], fmap.shape[3])) # Return empty CAM\n\n    # Grad-CAM++ specific calculations\n    grad2 = grad ** 2\n    grad3 = grad ** 3\n    alpha_num = grad2\n    alpha_denom = grad2 * fmap + grad3 * fmap ** 2\n    alpha_denom = torch.where(alpha_denom != 0, alpha_denom, torch.ones_like(alpha_denom)) # Avoid division by zero\n    alpha = alpha_num / alpha_denom\n\n    weights = torch.sum(alpha * torch.relu(grad), dim=(2, 3), keepdim=True)\n    cam = torch.sum(weights * fmap, dim=1)\n    cam = F.relu(cam) # Apply ReLU to CAM\n\n    # Normalize CAM to 0-1\n    cam = cam - cam.min()\n    cam = cam / (cam.max() + 1e-10) # Avoid division by zero\n\n    # Reset backbone.features gradients to original state (frozen)\n    for param in model.backbone.features.parameters():\n        param.requires_grad = False # Freeze lại nếu ban đầu bị đóng băng\n\n    return cam.cpu().numpy()[0]\n\n\ndef patch_uncertainty(fmap, patch_size=16):\n    # Ensure fmap is in float for variance calculation\n    fmap = fmap.float()\n    B, C, H, W = fmap.shape\n    # Sử dụng F.unfold thay vì unfold method để linh hoạt hơn\n    # Unfold along height (dim 2)\n    fmap_unfolded_h = F.unfold(fmap, kernel_size=(patch_size, 1), stride=(patch_size, 1))\n    # Unfold along width (dim 3)\n    fmap_unfolded_w = F.unfold(fmap_unfolded_h.transpose(2,3), kernel_size=(patch_size, 1), stride=(patch_size, 1))\n    fmap_patches = fmap_unfolded_w.transpose(2,3) # (B, C * patch_size * patch_size, num_patches)\n\n    # Reshape to (B, C, num_patches_h, num_patches_w, patch_size, patch_size)\n    fmap_patches = fmap_patches.view(B, C, H // patch_size, W // patch_size, patch_size, patch_size)\n\n    # Calculate variance across the patch dimensions (last two)\n    patch_var = fmap_patches.var(dim=(-2, -1)).mean(dim=1) # Mean over channels as well\n    return patch_var.view(-1, H // patch_size, W // patch_size)\n\n\ndef generate_heatmaps(model, img_tensor, target_class):\n    # Grad-CAM++ needs gradients from backbone\n    cam = grad_cam_plus_plus(model, img_tensor, target_class)\n    \n    # For uncertainty map, we need features, but no gradients are needed for UQ head's forward pass\n    model.eval() # Set model to eval mode for consistent behavior\n    with torch.no_grad(): # Ensure no gradients for uncertainty map generation\n        with autocast():\n            # Call forward_with_features for UQ map, but disable gradient tracking for it\n            fmap, _, evidence = model.forward_with_features(img_tensor.unsqueeze(0).to(device))\n    \n    unc_map = patch_uncertainty(fmap).cpu().numpy()[0]\n    unc_map = (unc_map - unc_map.min()) / (unc_map.max() + 1e-10)\n    return cam, unc_map\n\n# ---------------------------\n# 5. Train/Validate Functions\n# ---------------------------\ndef train_one_epoch(model, loader, cls_crit, opt, scaler, accumulation_steps, is_features_loader=False):\n    model.train()\n    tot_loss = tot_cls = tot_unc = 0; correct = 0; N = 0\n    opt.zero_grad()\n    for i, (x, y, _) in enumerate(loader): # _ for paths\n        x, y = x.to(device), y.to(device)\n        with autocast():\n            # Dựa vào input type để gọi model\n            if is_features_loader:\n                logits, evidence = model(x) # x đã là features\n            else:\n                logits, evidence = model(x) # x là ảnh\n\n            l_cls = cls_crit(logits, y)\n            l_unc = evidential_loss(logits, evidence, y)\n            loss = (l_cls + lambda_unc * l_unc) / accumulation_steps\n        scaler.scale(loss).backward()\n        if (i + 1) % accumulation_steps == 0:\n            scaler.step(opt)\n            scaler.update()\n            opt.zero_grad()\n        bs = x.size(0); N += bs\n        tot_loss += loss.item() * bs * accumulation_steps\n        tot_cls += l_cls.item() * bs\n        tot_unc += l_unc.item() * bs\n        correct += (logits.argmax(1) == y).sum().item()\n        torch.cuda.empty_cache()\n    return tot_loss / N, tot_cls / N, tot_unc / N, correct / N\n\ndef validate_one_epoch(model, loader, cls_crit, tau_conf):\n    model.eval()\n    tot_loss = 0; N = 0\n    all_conf = []; all_unc = []; all_true = []; all_pred = []; all_paths = []\n    with torch.no_grad():\n        for x, y, p in loader:\n            x, y = x.to(device), y.to(device)\n            with autocast():\n                logits, evidence = model(x) # Validate luôn dùng ảnh gốc để tính\n            probs = F.softmax(logits, dim=1); conf, _ = probs.max(1)\n            unc = compute_uncertainty(evidence).squeeze()\n            l_cls = cls_crit(logits, y)\n            tot_loss += l_cls.item() * x.size(0); N += x.size(0)\n            all_conf.append(np.atleast_1d(conf.cpu().numpy()))\n            all_unc.append(np.atleast_1d(unc.cpu().numpy()))\n            all_true.append(y.cpu().numpy())\n            all_pred.append(logits.argmax(1).cpu().numpy())\n            all_paths.extend(p)\n            torch.cuda.empty_cache()\n\n    confs = np.concatenate(all_conf); uncs = np.concatenate(all_unc)\n    trues = np.concatenate(all_true); preds = np.concatenate(all_pred)\n    \n    # Calculate metrics for LtR\n    rej = confs < tau_conf # Reject if confidence is below threshold\n    keep = ~rej\n    cov = keep.mean()\n    \n    # Handle cases where no samples are kept\n    corr_kept = ((preds == trues) & keep).sum()\n    risk = 1 - (corr_kept / keep.sum() if keep.sum() > 0 else 0.0) # Risk on kept samples\n    sel_acc = (corr_kept / keep.sum() if keep.sum() > 0 else 0.0) # Selective accuracy\n    \n    correct_rej = ((preds != trues) & rej).sum() # Correctly rejected (model was wrong and rejected)\n    false_rej = ((preds == trues) & rej).sum()   # Falsely rejected (model was right but rejected)\n    missed_rej = ((preds != trues) & keep).sum() # Missed rejections (model was wrong but kept)\n\n    # Limit heatmap generation to a few rejected samples for visualization\n    rejected_indices = np.where(rej)[0]\n    # Filter to only a few samples for practical visualization\n    if len(rejected_indices) > 0:\n        sample_indices_for_heatmaps = random.sample(list(rejected_indices), min(len(rejected_indices), 5)) # Limit to 5\n        print(f\"Generating heatmaps for {len(sample_indices_for_heatmaps)} rejected samples...\")\n        for i_original_dataset in sample_indices_for_heatmaps:\n            img_tensor = loader.dataset[i_original_dataset][0]\n            predicted_class = preds[i_original_dataset] # Use model's predicted class for CAM\n            \n            try:\n                cam, unc_map = generate_heatmaps(model, img_tensor, predicted_class)\n                original_filename = os.path.basename(all_paths[i_original_dataset]).split('.')[0]\n                plt.imsave(os.path.join(checkpoint_dir, f\"heatmap_cam_{original_filename}.png\"), cam, cmap='jet')\n                plt.imsave(os.path.join(checkpoint_dir, f\"heatmap_unc_{original_filename}.png\"), unc_map, cmap='jet')\n                print(f\"Saved heatmaps for {original_filename}\")\n            except Exception as e:\n                print(f\"Error generating heatmaps for {all_paths[i_original_dataset]}: {e}\")\n            torch.cuda.empty_cache() # Clear cache after each heatmap generation\n\n    return {\n        'loss': tot_loss / N,\n        'coverage': cov,\n        'selective_acc': sel_acc,\n        'risk': risk,\n        'correct_rejects': int(correct_rej),\n        'false_rejects': int(false_rej),\n        'missed_rejects': int(missed_rej),\n        'confidences': confs, 'uncertainties': uncs,\n        'true_labels': trues, 'predictions': preds,\n        'paths': all_paths\n    }\n\n# ----------------------------------------\n# NEW: Function to extract features for Phase 2\n# ----------------------------------------\ndef extract_features(model, loader, device):\n    model.eval()\n    all_features = []\n    all_labels = []\n    all_paths = []\n    print(\"Extracting features for Phase 2 training...\")\n    with torch.no_grad():\n        for i, (x, y, p) in enumerate(loader):\n            x = x.to(device)\n            with autocast():\n                # Forward pass through backbone only\n                f = model.backbone.features(x)\n                pooled_f = model.pool(f).flatten(1)\n            all_features.append(pooled_f.cpu())\n            all_labels.append(y.cpu())\n            all_paths.extend(p)\n            torch.cuda.empty_cache()\n            if (i+1) % 100 == 0:\n                print(f\"Processed {i+1} batches for feature extraction.\")\n    print(\"Feature extraction complete.\")\n    return torch.cat(all_features, dim=0), torch.cat(all_labels, dim=0), all_paths\n\n# ---------------------------\n# 6. Run Training\n# ---------------------------\ndef run_training():\n    # DataLoaders with reduced num_workers\n    train_tf = Compose([Resize(224,224), RandomBrightnessContrast(0.2,0.2,p=0.5),\n                         Rotate(40,p=0.5), HorizontalFlip(p=0.5),\n                         Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n                         ToTensorV2()])\n    test_tf  = Compose([Resize(224,224), Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]), ToTensorV2()])\n    \n    train_ds = CTScanDataset(train_pos, train_neg, train_tf)\n    test_ds  = CTScanDataset(test_pos, test_neg, test_tf) # Test set vẫn cần ảnh gốc cho validate\n    \n    train_ld_p1 = DataLoader(train_ds, batch_size_p1, shuffle=True, num_workers=2, pin_memory=True)\n    test_ld     = DataLoader(test_ds, batch_size_p1, shuffle=False, num_workers=2, pin_memory=True) # Test set luôn dùng ảnh gốc\n\n    # Model & criterions\n    model = MobileNetV2WithUncertainty().to(device)\n    cls_crit = nn.CrossEntropyLoss()\n    scaler = GradScaler()\n\n    # --- Phase 1: Train classifier ---\n    print(\"\\n--- Starting Phase 1: Training Classifier ---\")\n    # 1. Thu thập các tham số của unc_head vào một set để kiểm tra nhanh hơn\n    unc_head_params = set(model.unc_head.parameters())\n    \n    # 2. Đóng băng uncertainty head trong Pha 1\n    for p in unc_head_params: # Chỉ đóng băng các tham số CỦA unc_head\n        p.requires_grad = False\n    \n    # 3. Mở đóng băng cho các tham số khác (backbone và fc layer)\n    #    Để đảm bảo chỉ các phần mong muốn được huấn luyện, ta reset require_grad cho TẤT CẢ\n    #    các tham số trước, sau đó chỉ mở cho các phần mong muốn.\n    for p in model.parameters():\n        p.requires_grad = True # Mặc định mở lại tất cả\n    \n    #    Sau đó đóng băng lại những phần đã xác định là không huấn luyện\n    for p in unc_head_params:\n        p.requires_grad = False # Đóng băng lại unc_head\n    \n    # Khởi tạo optimizer chỉ với các tham số yêu cầu gradient\n    opt1 = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate_p1)\n    \n    best_selective_acc_p1 = 0 # Metric để lưu model tốt nhất cho P1\n    best_w1 = copy.deepcopy(model.state_dict())\n    \n    for e in range(num_epochs_p1):\n        t_loss, t_cls, t_unc, t_acc = train_one_epoch(model, train_ld_p1, cls_crit, opt1, scaler, accumulation_steps, is_features_loader=False)\n        val = validate_one_epoch(model, test_ld, cls_crit, 0.5) # Dùng 0.5 tạm thời để đánh giá\n        print(f\"[P1][Epoch {e+1}/{num_epochs_p1}] Train Loss: {t_loss:.4f} (CLS: {t_cls:.4f}, UNC: {t_unc:.4f}) Acc: {t_acc:.3f} | Val Cov: {val['coverage']:.3f} Sel Acc: {val['selective_acc']:.3f} Risk: {val['risk']:.3f}\")\n    \n        # Lưu model tốt nhất dựa trên Selective Accuracy trên tập validation\n        if val['selective_acc'] > best_selective_acc_p1:\n            best_selective_acc_p1 = val['selective_acc']\n            best_w1 = copy.deepcopy(model.state_dict())\n            print(f\"    New best P1 model saved with Sel Acc: {best_selective_acc_p1:.3f}\")\n    \n    model.load_state_dict(best_w1)\n    print(\"Phase 1 training complete. Loaded best model.\")\n    torch.cuda.empty_cache() # Giải phóng bộ nhớ sau Pha 1\n\n    # --- Optimize Temperature ---\n    # Tối ưu hóa nhiệt độ sau khi Pha 1 hoàn thành để hiệu chuẩn tốt hơn\n    optimize_temperature(model, test_ld, device)\n    torch.cuda.empty_cache()\n\n    # --- Feature Extraction for Phase 2 ---\n    # Sau khi P1 hoàn thành, trích xuất features từ tập train cho P2\n    train_features, train_labels, train_paths_features = extract_features(model, train_ld_p1, device)\n    # Tạo DataLoader mới cho Pha 2 từ các feature\n    train_ds_p2 = TensorDataset(train_features, train_labels, torch.tensor([i for i in range(len(train_labels))])) # Sử dụng index thay vì path tạm thời\n    # Tạo một DataLoader mới để truyền paths nếu bạn muốn, nhưng để đơn giản cho TensorDataset thì dùng index hoặc bỏ qua\n    train_ld_p2 = DataLoader(train_ds_p2, batch_size_p2, shuffle=True, num_workers=2, pin_memory=True)\n    \n    del train_features, train_labels # Giải phóng bộ nhớ của các tensor features không cần thiết nữa\n    torch.cuda.empty_cache()\n\n    # --- Phase 2: Train only Uncertainty Head ---\n    print(\"\\n--- Starting Phase 2: Training Uncertainty Head ---\")\n    # Đóng băng toàn bộ mô hình\n    for p in model.parameters():\n        p.requires_grad = False\n    # Chỉ mở đóng băng cho uncertainty head\n    for p in model.unc_head.parameters():\n        p.requires_grad = True\n    \n    # Tối ưu hóa chỉ các tham số của unc_head\n    opt2 = optim.Adam(model.unc_head.parameters(), lr=learning_rate_p2)\n    \n    best_risk_p2 = float('inf') # Tối ưu hóa Risk trong Pha 2\n    best_w2 = copy.deepcopy(model.state_dict())\n\n    for e in range(num_epochs_p2):\n        t_loss, t_cls, t_unc, t_acc = train_one_epoch(model, train_ld_p2, cls_crit, opt2, scaler, accumulation_steps, is_features_loader=True)\n        val = validate_one_epoch(model, test_ld, cls_crit, 0.5) # Dùng 0.5 để đánh giá tạm thời\n        print(f\"[P2][Epoch {e+1}/{num_epochs_p2}] Train Loss: {t_loss:.4f} (CLS: {t_cls:.4f}, UNC: {t_unc:.4f}) Acc: {t_acc:.3f} | Val Cov: {val['coverage']:.3f} Sel Acc: {val['selective_acc']:.3f} Risk: {val['risk']:.3f}\")\n        \n        # Lưu model tốt nhất dựa trên Risk trên tập validation (hoặc một metric khác bạn quan tâm cho LtR)\n        # Mục tiêu của LtR là giảm Risk trên các mẫu được giữ lại\n        if val['risk'] < best_risk_p2: # Smaller risk is better\n            best_risk_p2 = val['risk']\n            best_w2 = copy.deepcopy(model.state_dict())\n            print(f\"    New best P2 model saved with Risk: {best_risk_p2:.3f}\")\n            \n    model.load_state_dict(best_w2)\n    print(\"Phase 2 training complete. Loaded best model.\")\n    torch.cuda.empty_cache()\n\n    # --- Calibrate Conformal Threshold ---\n    # Tính nonconformity scores trên tập validation (ảnh gốc)\n    scores = compute_nonconformity(model, test_ld, device, is_features_loader=False)\n    tau_conf = set_conformal_threshold(scores, alpha_conf)\n    print(f\"Calibrated conformal threshold: tau_conf={tau_conf:.3f}\")\n\n    # --- Final Validation with Calibrated Threshold ---\n    print(\"\\n--- Performing Final Validation ---\")\n    final_val_results = validate_one_epoch(model, test_ld, cls_crit, tau_conf)\n    print(f\"Final Validation Metrics:\")\n    print(f\"  Coverage: {final_val_results['coverage']:.3f}\")\n    print(f\"  Selective Accuracy: {final_val_results['selective_acc']:.3f}\")\n    print(f\"  Risk on Kept Samples: {final_val_results['risk']:.3f}\")\n    print(f\"  Correct Rejects: {final_val_results['correct_rejects']}\")\n    print(f\"  False Rejects: {final_val_results['false_rejects']}\")\n    print(f\"  Missed Rejects: {final_val_results['missed_rejects']}\")\n\n    # Save final model\n    torch.save({'state': model.state_dict(), 'tau': tau_conf}, os.path.join(checkpoint_dir, \"final_model_with_tau.pth\"))\n    print(f\"Final model and conformal threshold saved to {os.path.join(checkpoint_dir, 'final_model_with_tau.pth')}\")\n    \n    return model, tau_conf\n\n# ---------------------------\n# Main\n# ---------------------------\nif __name__ == \"__main__\":\n    final_model, tc = run_training()\n    print(\"Done. Conformal Threshold:\", tc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-10T17:54:23.760468Z","iopub.execute_input":"2025-06-10T17:54:23.761878Z","execution_failed":"2025-06-10T18:24:13.730Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.8 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"},{"name":"stdout","text":"\n--- Starting Phase 1: Training Classifier ---\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-df8e72eb8d3b>:425: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()\n/usr/local/lib/python3.10/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n  warnings.warn(\n<ipython-input-1-df8e72eb8d3b>:287: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n  warnings.warn(\n<ipython-input-1-df8e72eb8d3b>:317: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"[P1][Epoch 1/10] Train Loss: 0.7232 (CLS: 0.4054, UNC: 0.6356) Acc: 0.818 | Val Cov: 1.000 Sel Acc: 0.938 Risk: 0.062\n    New best P1 model saved with Sel Acc: 0.938\n[P1][Epoch 2/10] Train Loss: 0.5403 (CLS: 0.2787, UNC: 0.5231) Acc: 0.889 | Val Cov: 1.000 Sel Acc: 0.952 Risk: 0.048\n    New best P1 model saved with Sel Acc: 0.952\n[P1][Epoch 3/10] Train Loss: 0.4631 (CLS: 0.2282, UNC: 0.4698) Acc: 0.908 | Val Cov: 1.000 Sel Acc: 0.946 Risk: 0.054\n[P1][Epoch 4/10] Train Loss: 0.4229 (CLS: 0.2005, UNC: 0.4449) Acc: 0.922 | Val Cov: 1.000 Sel Acc: 0.960 Risk: 0.040\n    New best P1 model saved with Sel Acc: 0.960\n[P1][Epoch 5/10] Train Loss: 0.4012 (CLS: 0.1867, UNC: 0.4290) Acc: 0.932 | Val Cov: 1.000 Sel Acc: 0.970 Risk: 0.030\n    New best P1 model saved with Sel Acc: 0.970\n[P1][Epoch 6/10] Train Loss: 0.3745 (CLS: 0.1666, UNC: 0.4158) Acc: 0.943 | Val Cov: 1.000 Sel Acc: 0.980 Risk: 0.020\n    New best P1 model saved with Sel Acc: 0.980\n[P1][Epoch 7/10] Train Loss: 0.3679 (CLS: 0.1618, UNC: 0.4121) Acc: 0.942 | Val Cov: 1.000 Sel Acc: 0.982 Risk: 0.018\n    New best P1 model saved with Sel Acc: 0.982\n[P1][Epoch 8/10] Train Loss: 0.3403 (CLS: 0.1409, UNC: 0.3989) Acc: 0.945 | Val Cov: 1.000 Sel Acc: 0.970 Risk: 0.030\n[P1][Epoch 9/10] Train Loss: 0.3410 (CLS: 0.1425, UNC: 0.3970) Acc: 0.952 | Val Cov: 1.000 Sel Acc: 0.986 Risk: 0.014\n    New best P1 model saved with Sel Acc: 0.986\n[P1][Epoch 10/10] Train Loss: 0.3108 (CLS: 0.1175, UNC: 0.3865) Acc: 0.957 | Val Cov: 1.000 Sel Acc: 0.982 Risk: 0.018\nPhase 1 training complete. Loaded best model.\nOptimizing temperature (initial: 1.001)...\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-1-df8e72eb8d3b>:156: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport random\n\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt\nfrom torchvision import models\nfrom albumentations import Compose, Resize, Normalize\nfrom albumentations.pytorch import ToTensorV2\n\n# ---------------------------\n# 0. Thiết lập chung\n# ---------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Đường dẫn tới model checkpoint đã train xong\nCHECKPOINT_PATH = \"./checkpoints/final_model.pth\"\n\n# Thư mục chứa ảnh test (COVID / non-COVID)\nTEST_POS_FOLDER = \"/kaggle/input/sarscov2-ctscan-dataset/COVID\"\nTEST_NEG_FOLDER = \"/kaggle/input/sarscov2-ctscan-dataset/non-COVID\"\n\n# Ngưỡng “reject” (sẽ được truyền vào từ training hoặc file)\n# Modified: Thresholds will be passed as arguments or loaded\nTAU_CONF = 0.6  # Placeholder, to be updated\nTAU_UNC  = 0.05  # Placeholder, to be updated\nBETA     = 0.5\n\n# ---------------------------\n# 1. Định nghĩa Model MobileNetV2WithUncertainty\n# ---------------------------\nclass MobileNetV2WithUncertainty(nn.Module):\n    def __init__(self, num_classes=2, p_dropout=0.5):  # Modified: Increased dropout rate to 0.5\n        super(MobileNetV2WithUncertainty, self).__init__()\n        backbone = models.mobilenet_v2(pretrained=True)\n        for idx, layer in enumerate(backbone.features):\n            if idx % 5 == 4:\n                backbone.features[idx].add_module(\"dropout\", nn.Dropout(p=p_dropout))\n\n        self.backbone = backbone\n        self.backbone.classifier = nn.Identity()\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        self.dropout = nn.Dropout(p=p_dropout)\n\n        self.fc = nn.Linear(self.backbone.last_channel, num_classes)\n        self.uncertainty_head = nn.Sequential(\n            nn.Linear(self.backbone.last_channel, 128),\n            nn.ReLU(),\n            nn.Dropout(p=p_dropout),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n\n        self.feature_grad = None\n\n    def save_feature_grad(self, grad):\n        self.feature_grad = grad\n\n    def forward(self, x):\n        features = self.backbone.features(x)\n        pooled = self.pool(features)\n        flat = torch.flatten(pooled, 1)\n        flat = self.dropout(flat)\n\n        logits = self.fc(flat)\n        u_pred = self.uncertainty_head(flat)\n        return logits, u_pred\n\n    def forward_with_features(self, x):\n        features = self.backbone.features(x)\n        if features.requires_grad:\n            features.register_hook(self.save_feature_grad)\n\n        fmap = features.clone()\n        pooled = self.pool(features)\n        flat = torch.flatten(pooled, 1)\n        flat = self.dropout(flat)\n\n        logits = self.fc(flat)\n        u_pred = self.uncertainty_head(flat)\n        return fmap, logits, u_pred\n\n\n# ---------------------------\n# 2. Hàm compute_gradcam & compute_uncertainty_cam\n# ---------------------------\ndef compute_gradcam(model, img_tensor):\n    model.eval()\n    fmap, logits, _ = model.forward_with_features(img_tensor.unsqueeze(0).to(device))\n    probs = F.softmax(logits, dim=1)[0]\n    class_idx = torch.argmax(probs).item()\n    top_conf = probs[class_idx].item()\n\n    model.zero_grad()\n    score = logits[0, class_idx]\n    score.backward(retain_graph=True)\n\n    grads = model.feature_grad[0]\n    fmap_data = fmap[0]\n    C, H, W = fmap_data.shape\n    weights = grads.view(C, -1).mean(dim=1)\n\n    cam = torch.zeros((H, W), dtype=torch.float32, device=fmap_data.device)\n    for k in range(C):\n        cam += weights[k] * fmap_data[k]\n    cam = torch.relu(cam).detach().cpu().numpy()\n\n    cam -= cam.min()\n    cam /= (cam.max() + 1e-8)\n\n    _, orig_h, orig_w = img_tensor.shape\n    cam_resized = cv2.resize(cam, (orig_w, orig_h))\n    return class_idx, top_conf, cam_resized\n\n\ndef compute_uncertainty_cam(model, img_tensor):\n    model.eval()\n    fmap, _, u_pred = model.forward_with_features(img_tensor.unsqueeze(0).to(device))\n    u_global = u_pred.item()\n\n    model.zero_grad()\n    u_pred.backward(retain_graph=True)\n\n    grads = model.feature_grad[0]\n    fmap_data = fmap[0]\n    C, H, W = fmap_data.shape\n    weights = grads.view(C, -1).mean(dim=1)\n\n    cam = torch.zeros((H, W), dtype=torch.float32, device=fmap_data.device)\n    for k in range(C):\n        cam += weights[k] * fmap_data[k]\n        \n    cam = torch.relu(cam).detach().cpu().numpy()\n\n    cam -= cam.min()\n    cam /= (cam.max() + 1e-8)\n\n    _, orig_h, orig_w = img_tensor.shape\n    cam_resized = cv2.resize(cam, (orig_w, orig_h))\n    return u_global, cam_resized\n\n\n# ---------------------------\n# 3. Hàm explain & reject\n# ---------------------------\ndef explain_ct_reject(model, img_tensor, tau_conf=TAU_CONF, tau_unc=TAU_UNC):  # Modified: Accept tau_conf, tau_unc as parameters\n    class_idx, top_conf, gradcam = compute_gradcam(model, img_tensor)\n    u_global, unc_cam = compute_uncertainty_cam(model, img_tensor)\n\n    if (top_conf < tau_conf) and (u_global > tau_unc):\n        composite = BETA * gradcam + (1 - BETA) * unc_cam\n        composite -= composite.min()\n        composite /= (composite.max() + 1e-8)\n        return {\n            \"reject\": True,\n            \"class_idx\": class_idx,\n            \"top_conf\": top_conf,\n            \"u_global\": u_global,\n            \"gradcam\": gradcam,\n            \"uncertainty_cam\": unc_cam,\n            \"composite_cam\": composite\n        }\n    else:\n        return {\n            \"reject\": False,\n            \"class_idx\": class_idx,\n            \"top_conf\": top_conf,\n            \"u_global\": u_global\n        }\n\n\n# ---------------------------\n# 4. Hàm visualize overlay (trên ảnh 224x224)\n# ---------------------------\ndef visualize_explanation(img_resized: np.ndarray, gradcam: np.ndarray,\n                         unc_cam: np.ndarray, composite: np.ndarray):\n    \"\"\"\n    img_resized: numpy array (224x224x3) không normalize, dtype uint8\n    Các heatmap: đều có shape (224,224) và giá trị trong [0,1]\n    \"\"\"\n    img_rgb = img_resized.copy()\n    if img_rgb.ndim == 2:\n        img_rgb = np.stack([img_rgb]*3, axis=-1)\n    # img_rgb chắc chắn đã có dạng 224x224x3 uint8\n\n    def overlay_heatmap(base, heatmap, cmap='jet', alpha=0.5):\n        heatmap_color = plt.get_cmap(cmap)(heatmap)[..., :3]  # (224,224,3) float [0,1]\n        base_float = base.astype(np.float32)\n        overlay = base_float * (1 - alpha) + heatmap_color * 255 * alpha\n        return overlay.astype(np.uint8)\n\n    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n    axes[0].imshow(img_rgb)\n    axes[0].set_title('Ảnh CT (224×224)'); axes[0].axis('off')\n\n    axes[1].imshow(overlay_heatmap(img_rgb, gradcam, cmap='jet', alpha=0.5))\n    axes[1].set_title('Grad-CAM'); axes[1].axis('off')\n\n    axes[2].imshow(overlay_heatmap(img_rgb, unc_cam, cmap='hot', alpha=0.5))\n    axes[2].set_title('Uncertainty-CAM'); axes[2].axis('off')\n\n    axes[3].imshow(overlay_heatmap(img_rgb, composite, cmap='jet', alpha=0.5))\n    axes[3].set_title('Composite'); axes[3].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n\n# ---------------------------\n# 5. Preprocess ảnh (Albumentations test)\n# ---------------------------\ntest_transform = Compose([\n    Resize(224, 224),\n    Normalize(mean=[0.485, 0.456, 0.406],\n              std=[0.229, 0.224, 0.225]),\n    ToTensorV2()\n])\n\ndef load_and_preprocess(path):\n    \"\"\"\n    Trả về:\n      - img_np_orig: numpy array RGB (H_orig x W_orig x 3), dtype uint8\n      - img_t: torch.Tensor (3 x 224 x 224), normalized để inference\n      - img_np_resized: numpy array RGB (224 x 224 x 3), dtype uint8\n    \"\"\"\n    # 1) Đọc ảnh gốc\n    img_bgr = cv2.imread(path)\n    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n    img_np_orig = img_rgb.copy()\n\n    # 2) Resize sang 224x224 để overlay\n    img_np_resized = cv2.resize(img_rgb, (224, 224))\n\n    # 3) Áp dụng Albumentations normalize + ToTensor (không áp dụng augmentation khác)\n    augmented = test_transform(image=img_np_resized)\n    img_t = augmented[\"image\"]  # Tensor (3,224,224)\n\n    return img_np_orig, img_t, img_np_resized\n\n\n# ---------------------------\n# 6. Hàm inference toàn tập Test\n# ---------------------------\ndef run_full_test_inference(model, tau_conf=TAU_CONF, tau_unc=TAU_UNC):  # Modified: Accept tau_conf, tau_unc as parameters\n    # Lấy tất cả ảnh test\n    pos_paths = glob.glob(os.path.join(TEST_POS_FOLDER, \"*.png\")) + \\\n                glob.glob(os.path.join(TEST_POS_FOLDER, \"*.jpg\"))\n    neg_paths = glob.glob(os.path.join(TEST_NEG_FOLDER, \"*.png\")) + \\\n                glob.glob(os.path.join(TEST_NEG_FOLDER, \"*.jpg\"))\n    all_test_paths = pos_paths + neg_paths\n\n    # Xây dictionary true label\n    true_labels = {}\n    for p in pos_paths:\n        true_labels[p] = 0\n    for p in neg_paths:\n        true_labels[p] = 1\n\n    total = len(all_test_paths)\n    rejected_count = 0\n    correct_rejections = 0\n    non_rejected_correct = 0\n    non_rejected_total = 0\n    misclassified_list = []\n    correct_rejection_list = []\n\n    print(f\"Tổng số ảnh test: {total}\")\n\n    for idx, img_path in enumerate(all_test_paths):\n        img_np_orig, img_t, img_np_resized = load_and_preprocess(img_path)\n        result = explain_ct_reject(model, img_t, tau_conf, tau_unc)  # Modified: Pass tau_conf, tau_unc\n        true_label = true_labels[img_path]\n\n        if result[\"reject\"]:\n            rejected_count += 1\n            if result[\"class_idx\"] != true_label:\n                # Correct rejection: misclassified and rejected\n                correct_rejections += 1\n                correct_rejection_list.append({\n                    \"path\": img_path,\n                    \"true\": true_label,\n                    \"pred\": result[\"class_idx\"],\n                    \"conf\": result[\"top_conf\"],\n                    \"unc\": result[\"u_global\"],\n                    \"reject\": True,\n                    \"gradcam\": result[\"gradcam\"],\n                    \"uncertainty_cam\": result[\"uncertainty_cam\"],\n                    \"composite_cam\": result[\"composite_cam\"],\n                    \"img_np_resized\": img_np_resized,\n                    \"img_np_orig\": img_np_orig\n                })\n            if result[\"class_idx\"] != true_label:\n                misclassified_list.append({\n                    \"path\": img_path,\n                    \"true\": true_label,\n                    \"pred\": result[\"class_idx\"],\n                    \"conf\": result[\"top_conf\"],\n                    \"unc\": result[\"u_global\"],\n                    \"reject\": True,\n                    \"gradcam\": result[\"gradcam\"],\n                    \"uncertainty_cam\": result[\"uncertainty_cam\"],\n                    \"composite_cam\": result[\"composite_cam\"],\n                    \"img_np_resized\": img_np_resized,\n                    \"img_np_orig\": img_np_orig\n                })\n        else:\n            non_rejected_total += 1\n            if result[\"class_idx\"] == true_label:\n                non_rejected_correct += 1\n            else:\n                misclassified_list.append({\n                    \"path\": img_path,\n                    \"true\": true_label,\n                    \"pred\": result[\"class_idx\"],\n                    \"conf\": result[\"top_conf\"],\n                    \"unc\": result[\"u_global\"],\n                    \"reject\": False,\n                    \"img_np_orig\": img_np_orig,\n                    \"img_np_resized\": img_np_resized\n                })\n\n        if (idx + 1) % 100 == 0 or (idx + 1) == total:\n            print(f\"  Đã chạy inference {idx+1}/{total} ảnh → Rejected: {rejected_count}\")\n\n    # Calculate accuracies\n    non_rejected_accuracy = (non_rejected_correct / non_rejected_total * 100) if non_rejected_total > 0 else 0\n    rejection_accuracy = (correct_rejections / rejected_count * 100) if rejected_count > 0 else 0\n\n    # Kết quả tổng hợp\n    print(f\"\\nKẾT QUẢ:\\n  Tổng số ảnh test            : {total}\")\n    print(f\"  Tổng số ảnh bị rejected     : {rejected_count}\")\n    print(f\"  Số ảnh reject đúng (misclassified và rejected): {correct_rejections}\")\n    print(f\"  Tỉ lệ reject đúng           : {rejection_accuracy:.2f}%\")\n    print(f\"  Số ảnh không rejected đúng  : {non_rejected_correct}/{non_rejected_total}\")\n    print(f\"  Tỉ lệ chính xác không rejected: {non_rejected_accuracy:.2f}%\")\n    print(f\"  Tổng số ảnh bị misclassified: {len(misclassified_list)}\\n\")\n\n    # Hiển thị các case reject đúng\n    if correct_rejection_list:\n        print(\"=== CÁC CASE REJECT ĐÚNG (MISCLASSIFIED VÀ REJECTED) ===\")\n        for case in correct_rejection_list:\n            img_path = case[\"path\"]\n            true_label = case[\"true\"]\n            pred = case[\"pred\"]\n            conf = case[\"conf\"]\n            unc = case[\"unc\"]\n\n            print(f\"--- Reject đúng: {os.path.basename(img_path)} ---\")\n            print(f\"  True Label = {true_label} | Predicted = {pred} | Conf = {conf:.3f} | Unc = {unc:.4f}\")\n            visualize_explanation(case[\"img_np_resized\"],\n                                 case[\"gradcam\"],\n                                 case[\"uncertainty_cam\"],\n                                 case[\"composite_cam\"])\n\n    # Hiển thị chi tiết các case misclassified\n    if misclassified_list:\n        print(\"=== CÁC CASE MISCLASSIFIED ===\")\n        for case in misclassified_list:\n            img_path = case[\"path\"]\n            true_label = case[\"true\"]\n            pred = case[\"pred\"]\n            conf = case[\"conf\"]\n            unc = case[\"unc\"]\n\n            print(f\"--- Sai nhãn: {os.path.basename(img_path)} ---\")\n            print(f\"  True Label = {true_label} | Predicted = {pred} | Conf = {conf:.3f} | Unc = {unc:.4f}\")\n\n            if case[\"reject\"]:\n                visualize_explanation(case[\"img_np_resized\"],\n                                     case[\"gradcam\"],\n                                     case[\"uncertainty_cam\"],\n                                     case[\"composite_cam\"])\n            else:\n                plt.figure(figsize=(4,4))\n                plt.imshow(case[\"img_np_orig\"])\n                plt.title(f\"Pred {pred} | Conf {conf:.3f}\")\n                plt.axis('off')\n                plt.show()\n\n    return rejected_count, misclassified_list, correct_rejections, non_rejected_accuracy, rejection_accuracy\n\n\n# ---------------------------\n# 7. Main: Load model và chạy Inference toàn tập Test\n# ---------------------------\nif __name__ == \"__main__\":\n    # Modified: Use optimal thresholds from training (example values, replace with actual values from training)\n    optimal_tau_conf = 0.75  # Example: Replace with value from training\n    optimal_tau_unc = 0.03   # Example: Replace with value from training\n\n    # Load model checkpoint\n    model = MobileNetV2WithUncertainty(num_classes=2).to(device)\n    if not os.path.exists(CHECKPOINT_PATH):\n        raise FileNotFoundError(f\"Không tìm thấy checkpoint tại {CHECKPOINT_PATH}\")\n    model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=device))\n    model.eval()\n    print(\"✔️ Đã load model từ:\", CHECKPOINT_PATH, \"\\n\")\n    print(f\"Using thresholds: tau_conf={optimal_tau_conf:.2f}, tau_unc={optimal_tau_unc:.4f}\\n\")\n\n    # Chạy inference trên toàn bộ test\n    rejected_count, misclassified_list, correct_rejections, non_rejected_accuracy, rejection_accuracy = run_full_test_inference(\n        model, tau_conf=optimal_tau_conf, tau_unc=optimal_tau_unc\n    )\n\n    print(\"\\nHoàn tất inference toàn tập test.\")\n    print(f\"Số lượng ảnh bị rejected     : {rejected_count}\")\n    print(f\"Số lượng ảnh reject đúng     : {correct_rejections}\")\n    print(f\"Tỉ lệ reject đúng            : {rejection_accuracy:.2f}%\")\n    print(f\"Tỉ lệ chính xác không rejected: {non_rejected_accuracy:.2f}%\")\n    print(f\"Số lượng ảnh bị misclassified: {len(misclassified_list)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport json\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n\nfrom torchvision import models\nfrom torch.utils.data import Dataset, DataLoader\nfrom albumentations import Compose, Resize, Normalize\nfrom albumentations.pytorch import ToTensorV2\n\n# ---------------------------\n# 0. Thiết lập chung\n# ---------------------------\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nCHECKPOINT_PATH = \"./checkpoints/final.pth\"\nTEST_POS_FOLDER = \"/kaggle/input/sarscov2-ctscan-dataset/COVID\"\nTEST_NEG_FOLDER = \"/kaggle/input/sarscov2-ctscan-dataset/non-COVID\"\n\n# ---------------------------\n# 1. Model definition (same as training script)\n# ---------------------------\nclass MobileNetV2WithUncertainty(nn.Module):\n    def __init__(self, num_classes=2, p_dropout=0.5):\n        super().__init__()\n        m = models.mobilenet_v2(pretrained=True)\n        for idx,layer in enumerate(m.features):\n            if idx % 5 == 4:\n                layer.add_module(\"dropout\", nn.Dropout(p_dropout))\n        self.backbone = m\n        self.backbone.classifier = nn.Identity()\n        self.pool = nn.AdaptiveAvgPool2d((1,1))\n        self.dropout = nn.Dropout(p_dropout)\n        self.fc = nn.Linear(self.backbone.last_channel, num_classes)\n        self.unc_head = nn.Sequential(\n            nn.Linear(self.backbone.last_channel,128),\n            nn.ReLU(),\n            nn.Dropout(p_dropout),\n            nn.Linear(128,1),\n            nn.Sigmoid()\n        )\n        self.feature_grad = None\n    def save_grad(self, grad): self.feature_grad = grad\n    def forward(self, x):\n        f = self.backbone.features(x)\n        p = self.pool(f).flatten(1)\n        p = self.dropout(p)\n        return self.fc(p), self.unc_head(p)\n    def forward_with_features(self, x):\n        f = self.backbone.features(x)\n        if f.requires_grad: f.register_hook(self.save_grad)\n        fmap = f.clone()\n        p = self.pool(f).flatten(1)\n        p = self.dropout(p)\n        return fmap, self.fc(p), self.unc_head(p)\n\n# ---------------------------\n# 2. Inference-time transforms\n# ---------------------------\ntest_transform = Compose([\n    Resize(224,224),\n    Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n    ToTensorV2()\n])\n\ndef load_and_preprocess(path):\n    img_bgr = cv2.imread(path)\n    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n    img_resized = cv2.resize(img_rgb, (224,224))\n    img_t = test_transform(image=img_resized)[\"image\"]\n    return img_rgb, img_resized, img_t\n\n# ---------------------------\n# 3. explain_ct_reject (same logic)\n# ---------------------------\ndef compute_gradcam(model, img_tensor):\n    model.zero_grad()\n    fmap, logits, _ = model.forward_with_features(img_tensor.unsqueeze(0).to(device))\n    probs = F.softmax(logits,1)[0]\n    class_idx = probs.argmax().item()\n    logits[0, class_idx].backward(retain_graph=True)\n    grads = model.feature_grad[0]\n    fmap  = fmap[0]\n    weights = grads.view(grads.size(0), -1).mean(dim=1)\n    cam = (weights.view(-1,1,1) * fmap).sum(0).relu().detach().cpu().numpy()\n    cam = (cam - cam.min())/(cam.max()-cam.min()+1e-8)\n    return class_idx, cam\n\ndef compute_uncertainty_cam(model, img_tensor):\n    model.zero_grad()\n    fmap, _, u_pred = model.forward_with_features(img_tensor.unsqueeze(0).to(device))\n    u = u_pred.item()\n    u_pred.backward(retain_graph=True)\n    grads = model.feature_grad[0]\n    fmap  = fmap[0]\n    weights = grads.view(grads.size(0),-1).mean(dim=1)\n    cam = (weights.view(-1,1,1) * fmap).sum(0).relu().detach().cpu().numpy()\n    cam = (cam - cam.min())/(cam.max()-cam.min()+1e-8)\n    return u, cam\n\ndef explain_ct_reject(model, img_tensor, tau_conf, tau_unc, beta=0.5):\n    class_idx, gradcam = compute_gradcam(model, img_tensor)\n    u, unc_cam = compute_uncertainty_cam(model, img_tensor)\n    with torch.no_grad():\n        logits,_ = model(img_tensor.unsqueeze(0).to(device))\n        conf = F.softmax(logits,1)[0,class_idx].item()\n    reject = (conf < tau_conf) and (u > tau_unc)\n    result = {\n        \"reject\": reject,\n        \"class_idx\": class_idx,\n        \"conf\": conf,\n        \"unc\": u\n    }\n    if reject:\n        # resize cams to 224x224\n        cam1 = cv2.resize(gradcam, (224,224))\n        cam2 = cv2.resize(unc_cam, (224,224))\n        comp = beta*cam1 + (1-beta)*cam2\n        result.update({\n            \"gradcam\": cam1,\n            \"uncertainty_cam\": cam2,\n            \"composite_cam\": (comp - comp.min())/(comp.max()-comp.min()+1e-8)\n        })\n    return result\n\n# ---------------------------\n# 4. visualize\n# ---------------------------\ndef overlay(base, heat, cmap, alpha=0.5):\n    heatc = plt.get_cmap(cmap)(heat)[...,:3]\n    return (base*(1-alpha) + heatc*255*alpha).astype(np.uint8)\n\ndef visualize_case(img, res):\n    fig,ax = plt.subplots(1,4,figsize=(16,4))\n    ax[0].imshow(img); ax[0].set_title(\"Orig\"); ax[0].axis(\"off\")\n    ax[1].imshow(overlay(img,res[\"gradcam\"],'jet')); ax[1].set_title(\"GradCAM\"); ax[1].axis(\"off\")\n    ax[2].imshow(overlay(img,res[\"uncertainty_cam\"],'hot')); ax[2].set_title(\"UncCAM\"); ax[2].axis(\"off\")\n    ax[3].imshow(overlay(img,res[\"composite_cam\"],'jet')); ax[3].set_title(\"Composite\"); ax[3].axis(\"off\")\n    plt.tight_layout(); plt.show()\n\n# ---------------------------\n# 5. Full inference + metrics\n# ---------------------------\ndef run_full_test(model, tau_conf, tau_unc):\n    pos = glob.glob(os.path.join(TEST_POS_FOLDER,\"*.png\"))\n    neg = glob.glob(os.path.join(TEST_NEG_FOLDER,\"*.png\"))\n    paths = pos+neg\n    trues = np.array([0]*len(pos) + [1]*len(neg))\n    confs = []; uncs = []; preds = []\n    for p in paths:\n        _,img224,img_t = load_and_preprocess(p)\n        res = explain_ct_reject(model, img_t, tau_conf, tau_unc)\n        confs.append(res[\"conf\"]); uncs.append(res[\"unc\"]); preds.append(res[\"class_idx\"])\n    confs = np.array(confs); uncs = np.array(uncs); preds=np.array(preds)\n    rej = (confs<tau_conf)&(uncs>tau_unc)\n    keep=~rej\n    total=len(paths)\n    RejCnt=rej.sum()\n    CorrRej = np.logical_and(rej, preds!=trues).sum()\n    FalseRej= np.logical_and(rej, preds==trues).sum()\n    KeptCorr= np.logical_and(keep, preds==trues).sum()\n    KeptTot = keep.sum()\n    sel_acc = KeptCorr/KeptTot if KeptTot>0 else 0\n    rej_acc = CorrRej/RejCnt if RejCnt>0 else 0\n    print(f\"Total: {total}, Rejected: {RejCnt}, CorrectRejects: {CorrRej}, RejectAcc: {rej_acc:.2%}\")\n    print(f\"Kept total: {KeptTot}, SelectiveAcc: {sel_acc:.2%}\")\n    # Display some correct rejects\n    for i,p in enumerate(paths):\n        if rej[i] and preds[i]!=trues[i]:\n            _,img224,_ = load_and_preprocess(p)\n            print(\"Correct reject:\", os.path.basename(p))\n            visualize_case(img224, explain_ct_reject(model, load_and_preprocess(p)[2], tau_conf, tau_unc))\n    # Display some misclassified-kept\n    for i,p in enumerate(paths):\n        if keep[i] and preds[i]!=trues[i]:\n            img_rgb,_,_ = load_and_preprocess(p)\n            print(\"Misclassified-kept:\", os.path.basename(p), \"pred\",preds[i],\"true\",trues[i])\n            plt.figure(figsize=(4,4)); plt.imshow(img_rgb); plt.axis(\"off\"); plt.show()\n\n# ---------------------------\n# 6. Main\n# ---------------------------\nif __name__ == \"__main__\":\n    # Load checkpoint: expect {'state':..., 'tau':(tc,tu)}\n    ckpt = torch.load(CHECKPOINT_PATH, map_location=device)\n    model = MobileNetV2WithUncertainty().to(device)\n    model.load_state_dict(ckpt['state'])\n    tau_conf, tau_unc = ckpt.get('tau', (0.85,0.04))\n    print(f\"Loaded model and thresholds: tau_conf={tau_conf:.2f}, tau_unc={tau_unc:.3f}\")\n\n    model.eval()\n    run_full_test(model, tau_conf, tau_unc)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}